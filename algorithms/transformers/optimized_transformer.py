#!/usr/bin/env python3
"""
Optimized SOTA Transformer with 125M Parameters
Enhanced version with proper configuration support and performance optimizations
"""

import torch
import torch.nn as nn
from typing import Optional, Union
import math
import time

from optimized_config import OptimizedSOTAConfig, get_config_125m
from sota_components.config import SOTAConfig
from sota_components.full_model import SOTATransformer as BaseSOTATransformer


class OptimizedSOTATransformer(BaseSOTATransformer):
    """
    Enhanced SOTA Transformer optimized for 125M parameter training.
    
    Key improvements:
    1. Support for OptimizedSOTAConfig
    2. Better parameter initialization
    3. Optimized generation with caching
    4. Memory monitoring
    5. Automatic mixed precision support
    """
    
    def __init__(self, config: Union[OptimizedSOTAConfig, SOTAConfig]):
        # Convert OptimizedSOTAConfig to SOTAConfig for compatibility
        if isinstance(config, OptimizedSOTAConfig):
            sota_config = self._convert_to_sota_config(config)
        else:
            sota_config = config
        
        # Store both configs
        self.optimized_config = config if isinstance(config, OptimizedSOTAConfig) else None
        
        # Initialize with base class
        super().__init__(sota_config)
        
        # Enhanced initialization for large models
        self._enhanced_init_weights()
        
        # Report enhanced metrics
        self._report_enhanced_metrics()
    
    def _convert_to_sota_config(self, opt_config: OptimizedSOTAConfig) -> SOTAConfig:
        """Convert OptimizedSOTAConfig to SOTAConfig for compatibility"""
        return SOTAConfig(
            vocab_size=opt_config.vocab_size,
            d_model=opt_config.d_model,
            num_heads=opt_config.num_heads,
            num_layers=opt_config.num_layers,
            d_ff=opt_config.d_ff,
            max_seq_len=opt_config.max_seq_len,
            dropout=opt_config.dropout,
            rope_base=opt_config.rope_base,
            eps=opt_config.eps,
            tie_embeddings=opt_config.tie_embeddings,
            use_flash_attn=opt_config.use_flash_attn,
            gradient_checkpointing=opt_config.gradient_checkpointing
        )
    
    def _enhanced_init_weights(self):
        """
        Enhanced weight initialization for large transformers.
        Uses scaled initialization similar to GPT-2.
        """
        # Get number of layers for scaling
        n_layers = self.config.num_layers
        
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                # Standard normal initialization
                std = 0.02
                
                # Scale down initializations for deeper layers
                if 'output_projection' in name or any(layer_name in name for layer_name in ['attn.o_proj', 'ffn.down_proj']):
                    # Scale by sqrt(2 * n_layers) for residual layers
                    std = 0.02 / math.sqrt(2 * n_layers)
                
                nn.init.normal_(module.weight, mean=0.0, std=std)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def _report_enhanced_metrics(self):
        """Report enhanced model metrics including memory usage"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # Estimate memory usage (rough approximation)
        param_memory_mb = total_params * 4 / (1024 * 1024)  # 4 bytes per float32
        
        print(f"\nðŸš€ OPTIMIZED SOTA TRANSFORMER")
        print("=" * 50)
        print(f"ðŸ“Š Model Size:")
        print(f"   â€¢ Total parameters: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"   â€¢ Trainable parameters: {trainable_params:,}")
        print(f"   â€¢ Parameter memory: ~{param_memory_mb:.1f} MB")
        
        print(f"\nðŸ—ï¸  Architecture:")
        print(f"   â€¢ Layers: {self.config.num_layers}")
        print(f"   â€¢ Model dimension: {self.config.d_model}")
        print(f"   â€¢ Attention heads: {self.config.num_heads}")
        print(f"   â€¢ Head dimension: {self.config.d_model // self.config.num_heads}")
        print(f"   â€¢ Feed-forward: {self.config.d_ff:,}")
        print(f"   â€¢ Vocabulary: {self.config.vocab_size:,}")
        print(f"   â€¢ Max sequence: {self.config.max_seq_len}")
        
        print(f"\nâš¡ Optimizations:")
        print(f"   â€¢ Tied embeddings: {self.config.tie_embeddings}")
        print(f"   â€¢ Flash attention: {self.config.use_flash_attn}")
        print(f"   â€¢ Gradient checkpointing: {self.config.gradient_checkpointing}")
        
        if self.optimized_config:
            print(f"\nðŸŽ¯ Training Config:")
            print(f"   â€¢ Learning rate: {self.optimized_config.learning_rate}")
            print(f"   â€¢ Weight decay: {self.optimized_config.weight_decay}")
            print(f"   â€¢ Batch size: {self.optimized_config.effective_batch_size}")
            print(f"   â€¢ Max steps: {self.optimized_config.max_steps:,}")
    
    @torch.no_grad()
    def generate_optimized(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100,
        temperature: float = 0.8,
        top_k: int = 50,
        top_p: float = 0.95,
        pad_token_id: Optional[int] = None
    ) -> torch.Tensor:
        """
        Optimized generation with proper padding handling.
        """
        self.eval()
        device = input_ids.device
        batch_size = input_ids.shape[0]
        
        # Set pad token
        if pad_token_id is None:
            pad_token_id = self.config.vocab_size - 1  # Use last token as pad
        
        generated_tokens = []
        
        for step in range(max_new_tokens):
            # Forward pass
            with torch.cuda.amp.autocast(enabled=False):  # Disable for MPS compatibility
                logits = self.forward(input_ids)
            
            # Get next token logits
            next_logits = logits[:, -1, :] / temperature
            
            # Apply top-k filtering
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))
                next_logits = torch.full_like(next_logits, float('-inf'))
                next_logits.scatter_(1, top_k_indices, top_k_logits)
            
            # Apply top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                
                # Remove tokens with cumulative probability above threshold
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = False
                
                indices_to_remove = torch.zeros_like(next_logits, dtype=torch.bool)
                indices_to_remove.scatter_(1, sorted_indices, sorted_indices_to_remove)
                next_logits[indices_to_remove] = float('-inf')
            
            # Sample next token
            probs = torch.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Append to sequence
            input_ids = torch.cat([input_ids, next_token], dim=1)
            generated_tokens.append(next_token)
            
            # Check for sequence length limit
            if input_ids.shape[1] >= self.config.max_seq_len:
                break
        
        return input_ids
    
    def get_memory_usage(self, device: torch.device) -> dict:
        """Get current memory usage statistics"""
        memory_stats = {}
        
        if device.type == 'cuda':
            memory_stats['allocated'] = torch.cuda.memory_allocated(device) / 1024**3
            memory_stats['cached'] = torch.cuda.memory_reserved(device) / 1024**3
            memory_stats['max_allocated'] = torch.cuda.max_memory_allocated(device) / 1024**3
        elif device.type == 'mps':
            # MPS doesn't have detailed memory stats yet
            memory_stats['allocated'] = 0
            memory_stats['cached'] = 0
            memory_stats['max_allocated'] = 0
        else:
            memory_stats['allocated'] = 0
            memory_stats['cached'] = 0
            memory_stats['max_allocated'] = 0
        
        return memory_stats


def create_125m_model(device: Optional[torch.device] = None) -> OptimizedSOTATransformer:
    """Create optimized 125M parameter model"""
    
    if device is None:
        if torch.backends.mps.is_available():
            device = torch.device('mps')
        elif torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
    
    print(f"ðŸ”§ Creating 125M parameter model on {device}")
    
    config = get_config_125m()
    model = OptimizedSOTATransformer(config).to(device)
    
    return model


def test_model_creation():
    """Test model creation and basic functionality"""
    print("ðŸ§ª TESTING OPTIMIZED TRANSFORMER MODEL")
    print("=" * 60)
    
    # Create model
    model = create_125m_model()
    
    # Test forward pass
    device = next(model.parameters()).device
    batch_size, seq_len = 2, 64
    
    print(f"\nðŸ”¬ Testing forward pass...")
    print(f"   Input shape: [{batch_size}, {seq_len}]")
    
    # Create test input
    input_ids = torch.randint(0, model.config.vocab_size, (batch_size, seq_len), device=device)
    
    start_time = time.time()
    with torch.no_grad():
        logits = model(input_ids)
    forward_time = time.time() - start_time
    
    print(f"   Output shape: {list(logits.shape)}")
    print(f"   Forward time: {forward_time:.4f}s")
    print(f"   Throughput: {batch_size * seq_len / forward_time:.0f} tokens/sec")
    
    # Test generation
    print(f"\nðŸ“ Testing generation...")
    prompt = torch.randint(0, 1000, (1, 10), device=device)  # Small vocab for test
    
    start_time = time.time()
    generated = model.generate_optimized(prompt, max_new_tokens=20, temperature=1.0)
    gen_time = time.time() - start_time
    
    print(f"   Generated tokens: {generated.shape[1] - prompt.shape[1]}")
    print(f"   Generation time: {gen_time:.4f}s")
    print(f"   Tokens/sec: {20 / gen_time:.1f}")
    
    # Memory usage
    memory_stats = model.get_memory_usage(device)
    if memory_stats['allocated'] > 0:
        print(f"\nðŸ’¾ Memory usage:")
        print(f"   Allocated: {memory_stats['allocated']:.2f} GB")
        print(f"   Cached: {memory_stats['cached']:.2f} GB")
    
    print(f"\nâœ… Model test complete!")
    return model


if __name__ == "__main__":
    model = test_model_creation()